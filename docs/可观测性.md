# 可观测性体系 (Metrics, Traces & Logs)

本项目的可观测性体系构建于 OpenTelemetry (OTel) 标准之上，集成了 Prometheus、Tempo 和 Grafana，为您提供了一套强大且统一的监控、追踪和日志聚合解决方案。

## 1. 核心组件与数据流

**数据流转路径**:

1.  **数据源 (Backend)**:
    *   `backend` 服务内嵌了 OpenTelemetry Node.js SDK。
    *   通过自动插桩 (`@opentelemetry/auto-instrumentations-node`)，SDK 会自动捕获所有传入的 HTTP 请求、Prisma 数据库查询等操作，将它们转换成 **Traces** (链路) 和 **Metrics** (指标)。
    *   所有遥测数据都遵循 OTLP (OpenTelemetry Protocol) 协议，被发送到统一的收集器。

2.  **数据收集 (OpenTelemetry Collector)**:
    *   `otel-collector` 是一个独立的服务，作为遥测数据的“中转站”和“处理器”。
    *   它通过 OTLP 接收器 (`receivers: otlp`) 监听来自 `backend` 的数据。
    *   通过处理器 (`processors`) 对数据进行批量处理和内存限制。
    *   通过导出器 (`exporters`) 将处理后的数据分发到不同的后端：
        *   **Traces** 被发送到 `Tempo`。
        *   **Metrics** 被发送到 `Prometheus`。

3.  **数据存储与查询**:
    *   **Tempo**: 一个专为海量链路追踪设计的高性能存储系统。它接收并存储所有 Trace 数据。
    *   **Prometheus**: 业界领先的时序数据库，负责存储所有 Metrics 数据（如请求速率、延迟、错误率等）。它会主动从 `otel-collector` 的 `/metrics` 端点拉取数据。

4.  **数据可视化 (Grafana)**:
    *   **Grafana** 是我们统一的可视化平台。
    *   它被预配置了两个核心数据源：
        *   **Prometheus 数据源**: 用于查询和展示指标数据，例如绘制 CPU 使用率、请求延迟 P99 等图表。
        *   **Tempo 数据源**: 用于查询和展示分布式链路。
    *   **关联体验**: 我们在 Grafana 中实现了 **Metrics-to-Traces** 的关联。当您在监控图表上看到一个异常的指标（例如延迟飙升）时，可以直接从该指标点跳转到相关的 Trace 详情，快速定位问题根源。

## 2. 如何启动

可观测性套件由独立的 Docker Compose 文件管理，以实现与主应用的解耦。

```bash
docker compose -f deploy/observability/docker-compose.yml up -d
```

启动后，您可以访问以下服务：

*   **Grafana**: `http://localhost:3000` (默认账号: `admin / admin`)
*   **Prometheus**: `http://localhost:9090`
*   **Tempo**: `http://localhost:3200` (主要由 Grafana 访问)

## 3. 后端 SDK 配置 (`tracing.ts`)

`backend` 服务中的 `tracing.ts` 是 OpenTelemetry SDK 的入口点。

*   **`NodeSDK`**: 初始化 SDK 的核心。
*   **`instrumentations`**:
    *   `getNodeAutoInstrumentations()`: 一个便捷的元包，包含了对 Node.js 核心模块 (http, fs等) 和许多流行框架 (Express, Koa) 的自动插桩。
    *   `PrismaInstrumentation()`: 专门用于捕获 Prisma 查询的插桩。
*   **环境变量驱动**: SDK 的导出器 (Exporter) 是通过环境变量动态配置的。在 `deploy/docker-compose.yml` 中，我们设置了：
    *   `OTEL_EXPORTER_OTLP_ENDPOINT`: 指向 `otel-collector` 的地址。
    *   `OTEL_TRACES_EXPORTER` / `OTEL_METRICS_EXPORTER`: 指定使用 `otlp` 协议。
    这使得代码与具体导出目标解耦，在不同环境中可以灵活切换。

## 4. Collector 配置 (`otel-collector.yml`)

`otel-collector` 的行为完全由 `otel-collector.yml` 定义，它由三个核心部分组成：

*   **`receivers`**: 定义如何接收数据。我们使用 `otlp` 接收器，同时监听 gRPC (`4317`) 和 HTTP (`4318`) 端口。
*   **`processors`**: 定义对数据的处理流程，例如 `batch` 处理器可以提高网络效率。
*   **`exporters`**: 定义数据发送到哪里。
    *   `otlp/tempo`: 将 trace 数据发送到 Tempo。
    *   `prometheus`: 暴露一个 `/metrics` 端点供 Prometheus 来抓取。
*   **`service.pipelines`**: 将 `receivers`, `processors`, `exporters` 串联起来，形成完整的数据处理流水线。

这种灵活的配置允许您未来轻松地添加更多的数据源或目标，例如将日志也发送到 Loki 或 ELK。
